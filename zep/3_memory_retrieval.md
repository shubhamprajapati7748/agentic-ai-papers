# Memory Retrieval 

To make this easy, Iâ€™ll use a **simple story**, and then show:

1. **What the user asks**
2. **What the retrieval system searches for**
3. **How multiple search methods bring different candidates**
4. **How the reranker decides which ones matter most**
5. **What final memory is returned to the LLM**

Letâ€™s dive in.

---

# ğŸ§  Scenario Setup (Knowledge Stored So Far)

Imagine the memory graph contains these facts:

### ğŸ’¬ Conversations (episodes)

* â€œI bought a **MacBook Air** last month.â€
* â€œI also got an **iPhone 15** last week.â€
* â€œMy **old Dell laptop** was too slow.â€
* â€œI prefer **lightweight laptops** for travel.â€
* â€œI might switch to a **gaming laptop** soon.â€

### ğŸ§© Entities & Facts

* User â†’ purchased â†’ MacBook Air (valid: last month)
* User â†’ purchased â†’ iPhone 15 (valid: last week)
* User â†’ previously_owned â†’ Dell Laptop
* User â†’ preference â†’ lightweight laptops
* User â†’ considering â†’ gaming laptop

---

# ğŸ¯ User Query

**â€œWhat laptop does the user currently use?â€**

The goal of retrieval is to fetch only the **most relevant memories**, *not the entire conversation*.

---

# ğŸ” SECTION 3 â€” RETRIEVAL WITH SEARCH + RERANKING (Illustrated)

Zepâ€™s retrieval pipeline has **3 stages**:

### **A. SEARCH**

Find all possibly relevant memories using multiple methods.

### **B. RERANKING**

Sort & filter them by relevance and importance.

### **C. CONSTRUCTION**

Format final facts/entities into a compact context for the LLM.

Letâ€™s break it down with our example.

---

# ğŸ…°ï¸ Step A: SEARCH (Multiple Search Methods)

Zep doesnâ€™t depend on *one* search method.
It runs **3 parallel searches** and merges their results.

---

## ğŸ”¸ 1. **Embedding Similarity Search**

Meaning-based search using vector embeddings.

Query:

> â€œWhat laptop does the user currently use?â€

Closest memories:

* â€œUser purchased **MacBook Air**â€
* â€œUser previously owned **Dell laptop**â€
* â€œUser prefers lightweight laptopsâ€

---

## ğŸ”¸ 2. **Full-Text Search (BM25)**

Keyword-based search on raw text.

Matching keywords:

* â€œlaptopâ€
* â€œMacBookâ€
* â€œDell laptopâ€
* â€œgaming laptopâ€

Candidates returned:

* â€œMy old Dell laptop was too slowâ€
* â€œI bought a MacBook Airâ€
* â€œI might switch to a gaming laptop soonâ€
* â€œI prefer lightweight laptopsâ€

---

## ğŸ”¸ 3. **Graph Search (BFS/Neighborhood Search)**

Graph traversal around key entities:

* Laptop
* User
* Purchase events
* Last known devices

Graph search returns:

* Edges linked to â€œUserâ€â€”â€œpurchasedâ€â€”â€œMacBook Airâ€
* Edges linked to â€œUserâ€â€”â€œownedâ€â€”â€œDell laptopâ€
* â€œpreference â†’ lightweight laptopsâ€

---

# ğŸŸ¦ Combined Candidate List

All candidates (unsorted for now):

1. Purchased MacBook Air
2. Old Dell laptop
3. Might switch to gaming laptop
4. Prefers lightweight laptops
5. Bought iPhone 15 (irrelevant)
6. Purchased last week (irrelevant)

---

# ğŸ…±ï¸ Step B: RERANKING (This is where magic happens)

Multiple reranking strategies are used:

### 1. **RRF (Reciprocal Rank Fusion)**

Combines ranks from all three searches to get balanced scoring.

### 2. **MMR (Maximal Marginal Relevance)**

Ensures diversity â€” avoids retrieving redundant facts.

### 3. **Graph distance weighting**

Facts directly connected to the user entity receive higher relevance.

### 4. **Entity mention frequency**

Frequently referenced entities are boosted.

### 5. **Cross-encoder scoring**

An LLM-based pairwise ranking for relevance to the question.

// Now letâ€™s see what happens.

---

## ğŸŸ© Reranking Example Outcome

After applying rerankers:

### **Top 1 (Most relevant)**

âœ” **User purchased MacBook Air (last month, still valid)**
Reason: Strong signal for â€œcurrently used laptopâ€

### **Top 2**

âœ” **User previously owned Dell laptop (expired fact)**
Useful as historical context, but ranked lower.

### **Top 3**

âœ” **User prefers lightweight laptops**
Useful for extra reasoning (MacBook Air is lightweight)

### **Excluded**

âŒ â€œI might switch to a gaming laptopâ€ â†’ low relevance
âŒ â€œiPhone 15â€ â†’ irrelevant
âŒ Minor conversations â†’ irrelevant

---

# ğŸ…² Step C: Context Construction

Now the system builds a clean, compact **context block** for the LLM.

---

## **ğŸ“ Final Constructed Memory Output**

*(This is exactly what Zep would return to the model)*

```
FACTS (Date range: Dec 2024 â€“ Jan 2025)
- The user purchased a MacBook Air last month.
- The user previously owned a Dell laptop, which was slow.
- The user prefers lightweight laptops.

ENTITIES
USER:
- Recently bought a MacBook Air.
- Prefers lightweight devices for travel.

MacBook Air:
- Userâ€™s current laptop.
- Purchased last month.

Dell Laptop:
- Previously used by the user.
- Was too slow, leading to upgrade.
```

Now if the LLM receives this context, it can confidently answer:

> â€œThe user currently uses a **MacBook Air**, which they purchased last month.â€
